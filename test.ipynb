{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "\n",
    "class DiffusionGeneration:\n",
    "    \"\"\"\n",
    "    Stable Diffusion for generation process.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, inpaint_pipe, refine_pipe, hp_dict, device):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            inpaint_pipe : Stable Diffusion inpaint pipeline. Note that: input size = 512\n",
    "            refine_pipe : Stable Diffusion refiner pipeline. Note that: input size = 1024\n",
    "            hp_dict (dict): Hyperparameters dicitionary for generation.\n",
    "            device (torch.device): Device used.\n",
    "        \"\"\"\n",
    "        # Setup device\n",
    "        self.device = device\n",
    "\n",
    "        # Setup pipelines\n",
    "        self.inpaint_pipe = inpaint_pipe.to(self.device)\n",
    "        self.refine_pipe = refine_pipe.to(self.device)\n",
    "\n",
    "        # Setup hyperparameters dictionary\n",
    "        self.hp_dict = hp_dict\n",
    "\n",
    "    def inpaint_generate_image(self, image, mask):\n",
    "        \"\"\"\n",
    "        Inpainting function\n",
    "\n",
    "        Args:\n",
    "            image (PIL.Image): Input image\n",
    "            mask (PIL.Image): Mask image\n",
    "\n",
    "        Returns:\n",
    "            inpainted_image (PIL.Image): Inpainted image\n",
    "        \"\"\"\n",
    "        # Save ori_size of input image to reconstruct\n",
    "        ori_size = image.size\n",
    "\n",
    "        # Resize image and mask to passing model\n",
    "        input_image = image.resize((1024, 1024))\n",
    "        input_mask = mask.resize((1024, 1024))\n",
    "\n",
    "        # Apply pipeline\n",
    "        generator = torch.Generator(self.device).manual_seed(self.hp_dict[\"seed\"])\n",
    "        result = self.inpaint_pipe(\n",
    "            image=input_image,\n",
    "            mask_image=input_mask,\n",
    "            guidance_scale=self.hp_dict[\"guidance_scale\"],\n",
    "            num_inference_steps=20,  # steps between 15 and 30 work well for us\n",
    "            strength=0.99,\n",
    "            prompt=self.hp_dict[\"prompt\"],\n",
    "            negative_prompt=self.hp_dict[\"negative_prompt\"],\n",
    "            generator=generator,\n",
    "        )\n",
    "        output_image = result.images[0]\n",
    "\n",
    "        # Resize inpainted image to original size\n",
    "        inpainted_image = output_image.resize(ori_size)\n",
    "\n",
    "        return inpainted_image\n",
    "\n",
    "    def refiner_generate_image(self, image, mask):\n",
    "        \"\"\"\n",
    "        Refiner function\n",
    "\n",
    "        Args:\n",
    "            image (PIL.Image): Input image with\n",
    "            mask (PIL.Image): Dilated mask image\n",
    "\n",
    "        Returns:\n",
    "            refined_image (PIL.Image): Refined image\n",
    "        \"\"\"\n",
    "        # Save ori_size of input image to reconstruct\n",
    "        ori_size = image.size\n",
    "\n",
    "        # Apply pipeline\n",
    "        result = self.refine_pipe(\n",
    "            prompt=self.hp_dict[\"prompt\"],\n",
    "            image=image,\n",
    "            mask_image=mask,\n",
    "            guidance_scale=self.hp_dict[\"guidance_scale\"],\n",
    "            num_inference_steps=self.hp_dict[\"num_inference_steps\"],\n",
    "            denoising_start=self.hp_dict[\"denoising_start\"],\n",
    "        )\n",
    "        output_image = result.images[0]\n",
    "\n",
    "        # Resize refined image to original size\n",
    "        refined_image = output_image.resize(ori_size)\n",
    "\n",
    "        return refined_image\n",
    "\n",
    "    def dilate_mask(self, init_mask):\n",
    "        \"\"\"\n",
    "        Make dilated mask from input mask\n",
    "\n",
    "        Args:\n",
    "            init_mask (np.array): Input mask\n",
    "\n",
    "        Returns:\n",
    "            dilated_mask_pil (PIL.Image): Dilated mask as PIL Image\n",
    "        \"\"\"\n",
    "        kernel = np.ones(self.hp_dict[\"kernel_size\"], np.uint8)\n",
    "        img_dilation = cv2.dilate(\n",
    "            init_mask, kernel, iterations=self.hp_dict[\"kernel_iterations\"]\n",
    "        )\n",
    "        dilated_mask = Image.fromarray(img_dilation)\n",
    "        return dilated_mask\n",
    "\n",
    "    def forward(self, image, mask, is_dilated=False):\n",
    "        \"\"\"\n",
    "        Generation process\n",
    "\n",
    "        Args:\n",
    "            image (PIL.Image): Input image\n",
    "            mask (PIL.Image): Mask image\n",
    "            is_dilated (bool): Check if generate dilated mask\n",
    "\n",
    "        Returns:\n",
    "            final_image (PIL.Image): Final result image\n",
    "        \"\"\"\n",
    "        # Inpaiting process\n",
    "        inpainted_image = self.inpaint_generate_image(image, mask)\n",
    "\n",
    "        # Get mask image\n",
    "        if is_dilated:\n",
    "            mask = self.dilate_mask(mask)\n",
    "\n",
    "        # Refining process\n",
    "        refined_image = self.refiner_generate_image(inpainted_image, mask)\n",
    "        return refined_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "from diffusers import StableDiffusionXLInpaintPipeline\n",
    "from diffusers import AutoPipelineForInpainting\n",
    "from diffusers.utils import load_image\n",
    "from diffusers import DiffusionPipeline\n",
    "from diffusers import DiffusionPipeline, LCMScheduler\n",
    "import torch\n",
    "\n",
    "model_id = \"stabilityai/stable-diffusion-xl-base-1.0\"\n",
    "lcm_lora_id = \"latent-consistency/lcm-lora-sdxl\"\n",
    "\n",
    "# Setup hyper parameters\n",
    "hp_dict = {\n",
    "    \"seed\": -305,\n",
    "    \"kernel_size\": (5, 5),\n",
    "    \"kernel_iterations\": 15,\n",
    "    \"num_inference_steps\": 70,\n",
    "    \"denoising_start\": 0.70,\n",
    "    \"guidance_scale\": 7.5,\n",
    "    \"prompt\": args.prompt,\n",
    "    \"negative_prompt\": args.negative_prompt,\n",
    "}\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Model Pipeline calling\n",
    "inpaint_pipe = AutoPipelineForInpainting.from_pretrained(\n",
    "    \"diffusers/stable-diffusion-xl-1.0-inpainting-0.1\",\n",
    "    torch_dtype=torch.float16,\n",
    "    variant=\"fp16\",\n",
    ")\n",
    "\n",
    "refine_pipe = StableDiffusionXLInpaintPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-xl-refiner-1.0\",\n",
    "    text_encoder_2=inpaint_pipe.text_encoder_2,\n",
    "    vae=inpaint_pipe.vae,\n",
    "    torch_dtype=torch.float16,\n",
    "    use_safetensors=True,\n",
    "    variant=\"fp16\",\n",
    ")\n",
    "\n",
    "inpaint_pipe.load_lora_weights(lcm_lora_id)\n",
    "inpaint_pipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)\n",
    "inpaint_pipe.to(device=\"cuda\", dtype=torch.float16)\n",
    "\n",
    "diffusion_gen = DiffusionGeneration(inpaint_pipe, refine_pipe, hp_dict, device)\n",
    "\n",
    "image = Image.open('/root/PhotoshopTools/BackGroundChanging/data/custom_dataset/Image.png')\n",
    "mask = Image.open('/root/PhotoshopTools/BackGroundChanging/mask/custom_dataset/Image.png')\n",
    "\n",
    "# Generate Image\n",
    "output_Image = diffusion_gen.forward(image=image, mask=ImageOps.invert(mask))\n",
    "output_Image.save(\"./output.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
